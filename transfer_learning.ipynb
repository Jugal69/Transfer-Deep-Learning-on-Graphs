{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transfer_learning.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8ggyCvh9vnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwjTfIeSoGjm",
        "colab_type": "code",
        "outputId": "54be8c68-a223-4110-846b-3b0c7d9668b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "directory='/content/gdrive/My Drive/Masters/DeepLearning/Project/Data'"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc_oM6VKiodH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def kernel_pca(x,nodes):\n",
        "  K = np.zeros((x.shape[1], x.shape[1]))\n",
        "  sigma = 0.1  # Hyperparameter of the RBF kernel\n",
        "  for i in range(x.shape[1]):  # Applying the RBF kernel on the input data\n",
        "    K[:, i] = np.exp(-np.sum((x[:, i].reshape((-1, 1))-x)**2, axis=0)/sigma**2)\n",
        "  u, s, v = np.linalg.svd(K,full_matrices=False)\n",
        "  return u[:nodes,:nodes]*s[:nodes]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7NHNmLE6g4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the F1 score: \n",
        "# Parameters: y=Actual label, yhat=Predicted label\n",
        "def getF1(y,yhat):\n",
        "  confusion={'TN':0, 'FP':np.count_nonzero(yhat[y!=yhat]),\n",
        "             'FN':0, 'TP':np.count_nonzero(y[y==yhat])+classes}\n",
        "  confusion['TN']=len(y[y==yhat])-confusion['TP']\n",
        "  confusion['FN']=len(y[y!=yhat])-confusion['FP']+classes\n",
        "  precision=confusion['TP']/(confusion['TP']+confusion['FP'])\n",
        "  recall=confusion['TP']/(confusion['TP']+confusion['FN'])\n",
        "  return 2*precision*recall/(precision+recall)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBDX6lnpjZ47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def padding(laplacian,nodes):\n",
        "  temp=np.zeros((nodes,nodes))\n",
        "  temp[:laplacian.shape[0],:laplacian.shape[1]]=laplacian\n",
        "  return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSYdTIXLHd1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2vec(corpus_raw,window_size,embedding_dim,nodes):\n",
        "  vector=np.zeros((len(corpus_raw),nodes,nodes))\n",
        "  for i in range(len(corpus_raw)):\n",
        "    words=set(corpus_raw[i])\n",
        "    print(i,len(words))\n",
        "    temp=np.zeros((len(words),embedding_dim))\n",
        "    if not corpus_raw[i] or len(corpus_raw[i])==1:\n",
        "      temp=np.random.rand(2,embedding_dim)\n",
        "    else:\n",
        "      word2vec=Word2Vec(corpus_raw[i],min_count=1,size=embedding_dim,window=window_size,workers=4)\n",
        "    count=0\n",
        "    for word in words:\n",
        "      temp[count]=word2vec.wv[word] if word in word2vec.wv else np.random.rand(embedding_dim)\n",
        "      count+=1\n",
        "    w=np.dot(temp,temp.T)\n",
        "#     graph=kernel_pca(temp,nodes) if len(words)!=nodes else temp\n",
        "    laplacian=np.diag(np.sum(w,axis=0))-w\n",
        "    vector[i]=kernel_pca(laplacian,nodes) if len(words)>nodes else padding(laplacian,nodes)\n",
        "  return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uypvOH8y-OlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes=2\n",
        "df1=pd.read_pickle(directory+'/reddit.pkl')\n",
        "corpus_raw=df1['cleanNews'].values.tolist()\n",
        "trainY=np.zeros((df1.shape[0],classes))\n",
        "trainY[:,df1['Change'].values]=1\n",
        "\n",
        "window_size=2\n",
        "embedding_dim=64\n",
        "nodes=24\n",
        "# vector = word2vec(corpus_raw,window_size,embedding_dim,nodes) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71l3ADqd-SV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.save(directory+'/vector.npy',vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZDL6BEgYSh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vector=np.load(directory+'/vector.npy')\n",
        "tf.reset_default_graph()  # To reset all the parameters of the graph for every execution\n",
        "minibatch_size=128  # Size of the minibatch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QgJvwr9dxsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filter1=8  # Number of filters\n",
        "kernel_size=3  # Filter size\n",
        "pad='same'  # Padding type\n",
        "filter2=16\n",
        "filter3=32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOIXur-vefH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pool_size=2\n",
        "pool_stride=2\n",
        "pool_pad='valid'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL7gBF30fDeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=tf.placeholder(tf.float32,shape=[None,nodes,nodes])\n",
        "y=tf.placeholder(tf.float32,shape=[None,classes])\n",
        "\n",
        "# Convolutional & Max-pooling Layer 1\n",
        "conv1=tf.layers.conv2d(tf.expand_dims(x,-1),filters=filter1, kernel_size = kernel_size, padding=pad,activation=tf.nn.relu, name=\"conv1\")\n",
        "pool1=tf.layers.max_pooling2d(conv1,pool_size=pool_size,strides=pool_stride,padding=pool_pad)\n",
        "\n",
        "# Convolutional & Max-pooling Layer 2\n",
        "conv2=tf.layers.conv2d(pool1,filters=filter2, kernel_size = kernel_size, padding=pad,activation=tf.nn.relu, name=\"conv2\")\n",
        "pool2=tf.layers.max_pooling2d(conv2,pool_size=pool_size,strides=pool_stride,padding=pool_pad)\n",
        "\n",
        "# Convolutional & Max-pooling Layer 3\n",
        "conv3=tf.layers.conv2d(pool2,filters=filter3, kernel_size = kernel_size, padding=pad,activation=tf.nn.relu, name=\"conv3\")\n",
        "pool3=tf.layers.max_pooling2d(conv3,pool_size=pool_size,strides=pool_stride,padding=pool_pad)\n",
        "\n",
        "fc=tf.layers.dense(tf.layers.flatten(pool3),classes,activation=tf.nn.softmax,name='fc')\n",
        "labels=tf.argmax(fc,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERad9KGsiLPW",
        "colab_type": "code",
        "outputId": "3345434f-3be5-4e2e-fe0b-49ecff8f214d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "source": [
        "loss=-tf.reduce_sum(y*tf.log(fc))  # Loss function:Mean squared error function\n",
        "train_step = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(loss)  # Adam Optimisation\n",
        "sess=tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "split=math.ceil(vector.shape[0]*0.7)\n",
        "indices=np.arange(vector.shape[0])\n",
        "np.random.shuffle(indices)  # Shuffle the order of the data\n",
        "maxEpoch=1001  # Total number of epochs\n",
        "for i in range(maxEpoch):\n",
        "  errt, _=sess.run([loss,train_step], feed_dict={x:vector[indices[:split]], y: trainY[indices[:split]]})\n",
        "  if not i%100:\n",
        "    print('Epoch number:',i,' Loss:',errt)\n",
        "accuracy=sess.run(tf.reduce_mean(tf.cast(tf.equal(labels,tf.argmax(y,axis=1)), tf.float32)), feed_dict={x:vector[indices[split:]], \n",
        "                                                                                                                 y:trainY[indices[split:]]})*100"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch number: 0  Loss: 385927.62\n",
            "Epoch number: 100  Loss: 209877.45\n",
            "Epoch number: 200  Loss: 113795.97\n",
            "Epoch number: 300  Loss: 78154.234\n",
            "Epoch number: 400  Loss: 62831.656\n",
            "Epoch number: 500  Loss: 55909.03\n",
            "Epoch number: 600  Loss: 53205.13\n",
            "Epoch number: 700  Loss: 52012.47\n",
            "Epoch number: 800  Loss: 51411.613\n",
            "Epoch number: 900  Loss: 51068.375\n",
            "Epoch number: 1000  Loss: 50843.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO0zu_xborAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2=pd.read_pickle(directory+'/demonetisation.pkl')\n",
        "corpus_raw1=df2['cleanText'].values.tolist()\n",
        "trainY=np.zeros((df2.shape[0],classes))\n",
        "trainY[:,df2['isRetweet'].values]=1\n",
        "# vector1 = word2vec(corpus_raw1,window_size,embedding_dim,nodes) \n",
        "# np.save(directory+'/vector1.npy',vector1)\n",
        "vector1=np.load(directory+'/vector1.npy')\n",
        "pool=sess.run(pool3,feed_dict={x:vector1})\n",
        "pool=pool.reshape((-1,288))\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fDhjxdqpNz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()  # To reset all the parameters of the graph for every execution\n",
        "p=tf.placeholder(tf.float32,shape=[None,288])\n",
        "y=tf.placeholder(tf.float32,shape=[None,classes])\n",
        "fc1=tf.layers.dense(p,72,activation=tf.nn.relu,name='fc1')\n",
        "fc2=tf.layers.dense(fc1,classes,activation=tf.nn.sigmoid,name='fc2')\n",
        "labels=tf.argmax(fc2,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_6YGuTLpfXE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "4930219c-899c-4324-919a-197b44a45bcf"
      },
      "source": [
        "loss=-tf.reduce_sum(y*tf.log(fc2))  # Loss function:Mean squared error function\n",
        "train_step = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)  # Adam Optimisation\n",
        "sess=tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "# split=math.ceil(vector1.shape[0]*0.7)\n",
        "# indices=np.arange(df2.shape[0])\n",
        "# np.random.shuffle(indices)  # Shuffle the order of the data\n",
        "split=vector1.shape[0]\n",
        "maxEpoch=101  # Total number of epochs\n",
        "for i in range(maxEpoch):\n",
        "  errt, _=sess.run([loss,train_step], feed_dict={p:pool[:split],y:trainY[:split]})\n",
        "  if not i%10:\n",
        "    print('Epoch number:',i,' Loss:',errt)\n",
        "test_pool=pool[:split]\n",
        "testY=np.argmax(trainY[:split],axis=1)\n",
        "predictY,dense2=sess.run([labels,fc2],feed_dict={p:test_pool})\n",
        "f1=getF1(testY,predictY)\n",
        "sess.close()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch number: 0  Loss: 23657.848\n",
            "Epoch number: 10  Loss: 0.77475035\n",
            "Epoch number: 20  Loss: 0.0911819\n",
            "Epoch number: 30  Loss: 0.036685508\n",
            "Epoch number: 40  Loss: 0.025082052\n",
            "Epoch number: 50  Loss: 0.021439958\n",
            "Epoch number: 60  Loss: 0.02004286\n",
            "Epoch number: 70  Loss: 0.01940101\n",
            "Epoch number: 80  Loss: 0.01902841\n",
            "Epoch number: 90  Loss: 0.018746994\n",
            "Epoch number: 100  Loss: 0.018497126\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}